#!/usr/bin/env bash

# Copy cache.xml to the geode cluster. It includes co-located data configuration
# for the /nw/orders region. Power BI files are located in etc/powerbi/ for generating
# reports using the co-located data.
GEODE_DOCKER_CLUSTER="geode"
echo ""
if [ -d $PADOGRID_WORKSPACE/docker/$GEODE_DOCKER_CLUSTER ]; then
   cp ../padogrid/etc/cache.xml $PADOGRID_WORKSPACE/docker/$GEODE_DOCKER_CLUSTER/padogrid/etc/cache.xml
   echo "Copied cache.xml to the $GEODE_DOCKER_CLUSTER docker cluster."
else
   echo "Failed to copy 'cache.xml' to the '$GEODE_DOCKER_CLUSTER' docker cluster because it does not exist."
   echo "This file configures Geode with co-located data. If want to co-locate data then either"
   echo "create a docker cluster named, '$GEODE_DOCKER_CLUSTER', or copy the provided 'cache.xml' file to"
   echo "your Geode docker cluster as follows:"
   echo ""
   echo "   cp ../padogrid/etc/cache.xml \$PADOGRID_WORKSPACE/docker/<geode_docker_cluster_name>/padogrid/etc/"
echo ""
fi


# Create nw database and grant all permissions to the user debezium
docker exec -e MYSQL_PWD=debezium -it debezium_hive_kafka_mysql_1 /bin/bash -c \
"mysql -uroot -e \"GRANT ALL PRIVILEGES ON nw.* TO 'debezium'@'%'; CREATE DATABASE IF NOT EXISTS nw;\""
echo "nw database created."

# Copy the Kafka handler jar to HDFS. Required for executing queries with joins.
# Queries are submitted as jobs to HDFS.
docker-compose exec hive /bin/bash -c \
"hdfs dfs -mkdir -p /opt/hive/lib && hdfs dfs -copyFromLocal /opt/hive/lib/kafka-handler-3.1.3000.7.2.0.2-2.jar /opt/hive/lib/"
echo "Hive updated."
echo ""
